{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experiment 2</h1>\n",
    "<h2>Test the performance of the proposed methodology on motion-corrupted data</h2>\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "packages that does conventional model fitting\n",
    "\"\"\"\n",
    "import amico\n",
    "\"\"\"\n",
    "packages that generate train/test dataset\n",
    "\"\"\"\n",
    "from FormatData import generate_data, parser as data_parser\n",
    "\"\"\"\n",
    "packages that trains network\n",
    "\"\"\"\n",
    "from Training import train_network\n",
    "from utils.model import parser as model_parser\n",
    "\"\"\"\n",
    "packages that test network\n",
    "\"\"\"\n",
    "from Testing import test_model\n",
    "\"\"\"\n",
    "packages that produce the rejection shceme\n",
    "\"\"\"\n",
    "from filter_qa import parser as filter_parser, load_eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    \"\"\"\n",
    "    a class generate parser for cmd line args\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "packages that handle graphs\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from utils import calc_ssim\n",
    "%matplotlib inline\n",
    "def plot_loss(cmd):\n",
    "    \"\"\"\n",
    "    A function that used to plot the loss curve for the trained network.\n",
    "    Args:\n",
    "        cmd: String, the command line in the terminal\n",
    "    \"\"\"\n",
    "    args = model_parser().parse_args(cmd.split())\n",
    "    history = train_network(args)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def show_slices(slices, grayscale=True):\n",
    "    \"\"\"\n",
    "    Function to display the slices\n",
    "\n",
    "    Args:\n",
    "        slices (list): a list of 2d ndarray that contains the data to be displayed\n",
    "        grayscale (bool, optional): True, if diplay grayscale img. Defaults to True.\n",
    "    \"\"\"    \n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(10,10))\n",
    "    cax = fig.add_axes([0, 0, .3, .3])\n",
    "    for i, slice in enumerate(slices):\n",
    "        # use grayscale for displaying ref and pred imgs:\n",
    "        if grayscale:\n",
    "            cmap = plt.get_cmap('gray')\n",
    "            cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "            cmap = LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            bounds = np.arange(0, 1.0, .01)\n",
    "            idx = np.searchsorted(bounds, 0)\n",
    "            bounds = np.insert(bounds, idx, 0)\n",
    "            norm = BoundaryNorm(bounds, cmap.N)\n",
    "            im = axes[i].imshow(slice.T, cmap=cmap, origin=\"lower\", interpolation='none', norm=norm)\n",
    "        else:\n",
    "            # define the colormap\n",
    "            cmap = plt.get_cmap('bwr')\n",
    "            # extract all colors from the .jet map\n",
    "            cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "            # create the new map\n",
    "            cmap = LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            # define the bins and normalize and forcing 0 to be part of the colorbar\n",
    "            # define the min and max to be -1 and +1 respectively\n",
    "            bounds = np.arange(-0.5, 0.5, .01)\n",
    "            idx = np.searchsorted(bounds, 0)\n",
    "            bounds = np.insert(bounds, idx, 0)\n",
    "            norm = BoundaryNorm(bounds, cmap.N)\n",
    "            im = axes[i].imshow(slice.T, cmap=cmap, origin=\"lower\", interpolation='none', norm=norm)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "def scale(img):\n",
    "    # for i in range(img.shape[0]):\n",
    "    #     for j in range(img.shape[1]):\n",
    "    #         img[i][j] = ((img[i][j]+1)/2)*255\n",
    "    return img\n",
    "\n",
    "def compare_simi(pred, ref):\n",
    "    return calc_ssim(pred, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(ref_ndi, ref_odi, ref_fwf, retained_vol, subject, model, layer, affine1, affine2, affine3):\n",
    "    \"\"\"\n",
    "    Function to visualise the imgs and difference maps\n",
    "\n",
    "    Args:\n",
    "        ref_ndi (ndarray): the reference NDI data\n",
    "        ref_odi (ndarray): the reference ODI data\n",
    "        ref_fwf (ndarray): the reference FWF data\n",
    "        retained_vol (int): the number of volumes retained after rejection\n",
    "        subject (string): the subject that is examined\n",
    "        model (string): the model used\n",
    "        layer (int): the number of layers for the network\n",
    "    \"\"\"\n",
    "    patch = 3\n",
    "    if model == 'fc1d':\n",
    "        patch = 1\n",
    "\n",
    "    print(patch)\n",
    "    print(model)\n",
    "    # visualise the ref imgs\n",
    "    refNDI0 = ref_ndi[26, :, :]\n",
    "    refNDI1 = ref_ndi[:, 30, :]\n",
    "    refNDI2 = ref_ndi[:, :, 16]\n",
    "    show_slices([refNDI0, refNDI1, refNDI2])\n",
    "    plt.suptitle(\"Center slices for NDI reference image\")\n",
    "\n",
    "    refODI0 = ref_odi[26, :, :]\n",
    "    refODI1 = ref_odi[:, 30, :]\n",
    "    refODI2 = ref_odi[:, :, 16]\n",
    "    show_slices([refODI0, refODI1, refODI2])\n",
    "    plt.suptitle(\"Center slices for ODI reference image\")\n",
    "\n",
    "    refFWF0 = ref_fwf[26, :, :]\n",
    "    refFWF1 = ref_fwf[:, 30, :]\n",
    "    refFWF2 = ref_fwf[:, :, 16]\n",
    "    show_slices([refFWF0, refFWF1, refFWF2])\n",
    "    plt.suptitle(\"Center slices for FWF reference image\")\n",
    "\n",
    "    # visualise the pred imgs produced at varied input size\n",
    "    ndi_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_NDI.nii'\n",
    "    odi_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_ODI.nii'\n",
    "    fwf_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_FWF.nii'\n",
    "    ndi_img = nib.load(ndi_path)\n",
    "    ndi_data = ndi_img.get_fdata()\n",
    "    odi_img = nib.load(odi_path)\n",
    "    odi_data = odi_img.get_fdata()\n",
    "    fwf_img = nib.load(fwf_path)\n",
    "    fwf_data = fwf_img.get_fdata()\n",
    "\n",
    "    ndi0 = ndi_data[26, :, :]\n",
    "    ndi1 = ndi_data[:, 30, :]\n",
    "    ndi2 = ndi_data[:, :, 16]\n",
    "    show_slices([ndi0, ndi1, ndi2])\n",
    "    plt.suptitle('Center slices for NDI predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, ndidiff) = compare_ssim(ndi_data, ref_ndi, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "    odi0 = odi_data[26, :, :]\n",
    "    odi1 = odi_data[:, 30, :]\n",
    "    odi2 = odi_data[:, :, 16]\n",
    "    show_slices([odi0, odi1, odi2])\n",
    "    plt.suptitle('Center slices for ODI predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, odidiff) = compare_ssim(odi_data, ref_odi, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for odi is: ' + str(score))\n",
    "\n",
    "    fwf0 = fwf_data[26, :, :]\n",
    "    fwf1 = fwf_data[:, 30, :]\n",
    "    fwf2 = fwf_data[:, :, 16]\n",
    "    show_slices([fwf0, fwf1, fwf2])\n",
    "    plt.suptitle('Center slices for FWF predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, fwfdiff) = compare_ssim(fwf_data, ref_fwf, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "    # plot the difference map between the imgs by the lib\n",
    "    ndidiff0 = scale(refNDI0 - ndi0)\n",
    "    ndidiff1 = scale(refNDI1 - ndi1)\n",
    "    ndidiff2 = scale(refNDI2 - ndi2)\n",
    "    show_slices([ndidiff0, ndidiff1, ndidiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map NDI\")\n",
    "\n",
    "    diff_img_np = ref_ndi - ndi_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine1)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_NDI_difference.nii')\n",
    "\n",
    "    odidiff0 = scale(refODI0 - odi0)\n",
    "    odidiff1 = scale(refODI1 - odi1)\n",
    "    odidiff2 = scale(refODI2 - odi2)\n",
    "    show_slices([odidiff0, odidiff1, odidiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map ODI\")\n",
    "\n",
    "    diff_img_np = ref_odi - odi_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine2)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_ODI_difference.nii')\n",
    "\n",
    "    fwfdiff0 = scale(refFWF0 - fwf0)\n",
    "    fwfdiff1 = scale(refFWF1 - fwf1)\n",
    "    fwfdiff2 = scale(refFWF2 - fwf2)\n",
    "    show_slices([fwfdiff0, fwfdiff1, fwfdiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map FWF\")\n",
    "\n",
    "    diff_img_np = ref_fwf - fwf_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine3)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_FWF_difference.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion-free subject path\n",
    "s01_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/'\n",
    "s02_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/'\n",
    "s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/'\n",
    "s04_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/'\n",
    "# motion-free target labels\n",
    "s01_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_NDI.nii'\n",
    "s02_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_NDI.nii'\n",
    "s03_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_NDI.nii'\n",
    "s04_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_NDI.nii'\n",
    "\n",
    "s01_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_ODI.nii'\n",
    "s02_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_ODI.nii'\n",
    "s03_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_ODI.nii'\n",
    "s04_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_ODI.nii'\n",
    "\n",
    "s01_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_FWF.nii'\n",
    "s02_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_FWF.nii'\n",
    "s03_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_FWF.nii'\n",
    "s04_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_FWF.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mask(subpath, fwfpath, threshold=0.99):\n",
    "    \"\"\"\n",
    "    By looking at the imgs generated, we have found out there are some regions that should not be included. Since they have values higher than 1.0\n",
    "    And we have found out voxels have NDI and ODI values, while that voxel has GROUND TRUTH FWF 1.0\n",
    "    This should indicate that that voxel should not even be included in the training\n",
    "    Therefore we want to filter the each subject's mask first, by using their corresponding GROUND TRUTH FWF\n",
    "\n",
    "    Args:\n",
    "        subpath (string): the path of the subject folder\n",
    "        fwfpath (string): the path of the corresponding fwf file\n",
    "        threshold (float): the thresholds to be used to filter of the mask,\n",
    "                           a stringnent threshold would be 0.9, the least stringnent threshold is 1.0\n",
    "                           by default, it is set to 0.99\n",
    "    \"\"\"\n",
    "    # fetch the mask data\n",
    "    img_mask = nib.load(subpath+'mask-e.nii')\n",
    "    original_mask = img_mask.get_fdata()\n",
    "    original_affine = img_mask.affine\n",
    "    shape = original_mask.shape # retain the shape of the mask\n",
    "    origin_nonzeros = np.count_nonzero(original_mask)\n",
    "    print('original mask has: ' + str(origin_nonzeros) + ' of nonzero voxels')\n",
    "    # fetch the FWF data\n",
    "    fwf = nib.load(fwfpath).get_fdata()\n",
    "    # filter\n",
    "    mask = original_mask.flatten() # this makes a copy of the orginal mask\n",
    "    fwf = fwf.reshape(mask.shape[0]) # reshape fwf to the corresponding shape\n",
    "    for i in range(len(mask)):\n",
    "        # if fwf has high value, means there is no tissue\n",
    "        # therefore, the voxel should be excluded\n",
    "        if fwf[i] >= threshold:\n",
    "            mask[i] = 0.0\n",
    "    # reshape mask back\n",
    "    mask = mask.reshape(shape)\n",
    "    filter_nonzeros = np.count_nonzero(mask)\n",
    "    print('filtered mask has: ' +str(filter_nonzeros) + ' of nonzero voxels')\n",
    "    # save the mask\n",
    "    filter_img = nib.Nifti1Image(mask, original_affine)\n",
    "    nib.save(filter_img, subpath+'filtered_mask.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above code to filter each subject's mask. Store as filtered_mask.nii in each subject folder\n",
    "filter_mask(s01_path, s01_FWF_path)\n",
    "filter_mask(s02_path, s02_FWF_path)\n",
    "filter_mask(s03_path, s04_FWF_path)\n",
    "filter_mask(s04_path, s03_FWF_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered mask path for each subject\n",
    "s01_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/filtered_mask.nii'\n",
    "s02_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/filtered_mask.nii'\n",
    "s03_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/filtered_mask.nii'\n",
    "s04_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/filtered_mask.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate the base dataset for s01_still, s02_still, s03_still_reg and s04_still_reg first for all NODDI parameters.\n",
    "\"\"\"\n",
    "cmd = \"--base --label_type A --subjects s01_still s02_still s03_still_reg s04_still_reg\"\n",
    "args = data_parser().parse_args(cmd.split())\n",
    "generate_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using nib to fetch the ground truth img\n",
    "\"\"\"\n",
    "#load the truth data for subject 1\n",
    "s01_NDI_img = nib.load(s01_NDI_path)\n",
    "s01_ODI_img = nib.load(s01_ODI_path)\n",
    "s01_FWF_img = nib.load(s01_FWF_path)\n",
    "s01_NDI_affine = s01_NDI_img.affine\n",
    "s01_ODI_affine = s01_ODI_img.affine\n",
    "s01_FWF_affine = s01_FWF_img.affine\n",
    "s01_NDI_img_data = s01_NDI_img.get_fdata()\n",
    "s01_ODI_img_data = s01_ODI_img.get_fdata()\n",
    "s01_FWF_img_data = s01_FWF_img.get_fdata()\n",
    "#load the truth data for subject 2\n",
    "s02_NDI_img = nib.load(s02_NDI_path)\n",
    "s02_ODI_img = nib.load(s02_ODI_path)\n",
    "s02_FWF_img = nib.load(s02_FWF_path)\n",
    "s02_NDI_affine = s02_NDI_img.affine\n",
    "s02_ODI_affine = s02_ODI_img.affine\n",
    "s02_FWF_affine = s02_FWF_img.affine\n",
    "s02_NDI_img_data = s02_NDI_img.get_fdata()\n",
    "s02_ODI_img_data = s02_ODI_img.get_fdata()\n",
    "s02_FWF_img_data = s02_FWF_img.get_fdata()\n",
    "# load the truth data for subject 3\n",
    "s03_NDI_img = nib.load(s03_NDI_path)\n",
    "s03_ODI_img = nib.load(s03_ODI_path)\n",
    "s03_FWF_img = nib.load(s03_FWF_path)\n",
    "s03_NDI_affine = s03_NDI_img.affine\n",
    "s03_ODI_affine = s03_ODI_img.affine\n",
    "s03_FWF_affine = s03_FWF_img.affine\n",
    "s03_NDI_img_data = s03_NDI_img.get_fdata()\n",
    "s03_ODI_img_data = s03_ODI_img.get_fdata()\n",
    "s03_FWF_img_data = s03_FWF_img.get_fdata()\n",
    "# load the truth data for subject 4\n",
    "s04_NDI_img = nib.load(s04_NDI_path)\n",
    "s04_ODI_img = nib.load(s04_ODI_path)\n",
    "s04_FWF_img = nib.load(s04_FWF_path)\n",
    "s04_NDI_affine = s04_NDI_img.affine\n",
    "s04_ODI_affine = s04_ODI_img.affine\n",
    "s04_FWF_affine = s04_FWF_img.affine\n",
    "s04_NDI_img_data = s04_NDI_img.get_fdata()\n",
    "s04_ODI_img_data = s04_ODI_img.get_fdata()\n",
    "s04_FWF_img_data = s04_FWF_img.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the proposed networks, 4 layer ANN, 2D CNN and 3D CNN, to carry out this experiment. The networks are required to be trained again, since this time we will use the rejection scheme to select the input DWI rather than using the sequential scheme.<br/>\n",
    "To use the rejection scheme, we are firstly required to use filter_qa.py to set up the thresholds to obtain the retained volumes.<br/>\n",
    "<h4>Filtering the data</h4><br/>\n",
    "To filter the data, user needs to provide the path for s03_motion and s04_motion, by that filter_qa can access the QAfrom-eddylog.txt. Using the thresholds set by the users, the code could produce a rejection scheme file that has binary values in it. Where 1 represents the one has motion-level below the thresholds; whereas 0 represents the one has higher motion-level than the thresholds. Hence, when the rejection scheme is applied to the (training/testing) datasets, the code would select the volumes having corresponding 1s in the scheme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the motion data\n",
    "s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion' # the path of subject 3 motion\n",
    "s04_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion' # the path of subject 3 motion\n",
    "# set the thresholds, and the number of DWIs to be examined. THe thresholds are all float numbers\n",
    "# stringnet thresholds are used t0=Absolute Translation, t1=Relative Translation, r0=Absolute Rotation, r1=Relative Rotation and out=Fractional Signal Dropouts\n",
    "t0, t1, r0, r1, out, num = 3.0, 2.0, 3.0, 2.0, 0.05, 96 \n",
    "# produce the filter cmd and args\n",
    "filter_cmd = '--path ' + s03_path + ' --t0 ' + str(t0) + ' --t1 ' + str(t1) + ' --r0 ' + str(r0) + ' --r1 ' + str(r1) + ' --out ' + str(out) + ' --num ' + str(num)\n",
    "filter_args = filter_parser().parse_args(filter_cmd.split())\n",
    "# this should generate move_t0-{t0}_t1-{t1}_r0-{r0}_r1-{r1}_out-{out}.txt file in the path of subject 3 motion for s03 first\n",
    "load_eddy(filter_args)\n",
    "\n",
    "filter_cmd = '--path ' + s04_path + ' --t0 ' + str(t0) + ' --t1 ' + str(t1) + ' --r0 ' + str(r0) + ' --r1 ' + str(r1) + ' --out ' + str(out) + ' --num ' + str(num)\n",
    "filter_args = filter_parser().parse_args(filter_cmd.split())\n",
    "# rejection scheme for s04 is generated\n",
    "load_eddy(filter_args)\n",
    "\n",
    "# the path of the rejection schemes\n",
    "s03_movefile = s03_path + '/move_t0-'+str(t0)+'_t1-'+str(t1)+'_r0-'+str(r0)+'_r1-'+str(r1)+'_out-'+str(out)+'.txt'\n",
    "s04_movefile = s04_path + '/move_t0-'+str(t0)+'_t1-'+str(t1)+'_r0-'+str(r0)+'_r1-'+str(r1)+'_out-'+str(out)+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training</h4><br/>\n",
    "Now, we will use the rejection scheme we have, to fetch the training data for training our proposed networks.<br/>\n",
    "Firstly, we will define the hyperparameters that will be used for all proposed networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the networks\n",
    "layers = 4 # the number of hidden layers; this is the optimal number of layer\n",
    "lr = 0.0001 # the learning rate\n",
    "patch_size = 3 # the size of patches for 2D and 3D CNN\n",
    "batch = 256 # the batch size\n",
    "epoch = 100 # the number of epoches for training\n",
    "ltype = 'A' # the NODDI parameter we want to estimate, where A stands for ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>ANN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'fc1d'\n",
    "# firstly, the ann dataset is required to be generated.\n",
    "# the cmd line code for generating ANN data\n",
    "anndata_cmd = '--subjects s01_still s02_still s03_still_reg s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "anndata_args  = data_parser().parse_args(anndata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-1d-1-1-all.mat ... s04_still_reg-base1-patches-1d-1-1-all.mat under Net/datasets/data\n",
    "generate_data(anndata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "anntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s03_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(anntrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained ANN by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "anntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s03_movefile\n",
    "anntest_args = model_parser().parse_args(anntest_cmd.split())\n",
    "# test\n",
    "test_model(anntest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers,affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "study_subject = 's04_still_reg' # this is the name of the study subject\n",
    "sep_train_subject = 's01_still' # this is the name of the separate training candidate\n",
    "# we use the rejection scheme produced from the study subject to select the desired subset from the separate training dataset. And we train on the separate training dataset\n",
    "anntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(anntrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained ANN by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "anntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "anntest_args = model_parser().parse_args(anntest_cmd.split())\n",
    "# test\n",
    "test_model(anntest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>2D CNN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'conv2d'\n",
    "# firstly, the 2d cnn dataset is required to be generated.\n",
    "# the cmd line code for generating 2d cnn data\n",
    "cnn2ddata_cmd = '--subjects s01_still s02_still s03_still_reg s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "cnn2ddata_args  = data_parser().parse_args(cnn2ddata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-2d-3-1-all.mat ... s04_still_reg-base1-patches-3d-3-1-all.mat under Net/datasets/data\n",
    "generate_data(cnn2ddata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn2dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s03_movefile + ' --train'\n",
    "# train 2d cnn and plot the loss curve\n",
    "plot_loss(cnn2dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 2d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn2dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s03_movefile\n",
    "cnn2dtest_args = model_parser().parse_args(cnn2dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn2dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process for study subject s04\n",
    "# we study subject s04, by using s01 as a seperate training dataset\n",
    "study_subject = 's04_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn2dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(cnn2dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 2d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn2dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "cnn2dtest_args = model_parser().parse_args(cnn2dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn2dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>3D CNN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'conv3d'\n",
    "# firstly, the 3d cnn dataset is required to be generated.\n",
    "# the cmd line code for generating 3d cnn data\n",
    "cnn3ddata_cmd = '--subjects s01_still s02_still s03_still_reg s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "cnn3ddata_args  = data_parser().parse_args(cnn3ddata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-3d-3-1-all.mat ... s04_still_reg-base1-patches-3d-3-1-all.mat under Net/datasets/data\n",
    "generate_data(cnn3ddata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn3dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s03_movefile + ' --train'\n",
    "# train 3d cnn and plot the loss curve\n",
    "plot_loss(cnn3dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 3d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn3dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s03_movefile\n",
    "cnn3dtest_args = model_parser().parse_args(cnn3dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn3dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "# we study subject s04, by using s01 as a seperate training dataset\n",
    "study_subject = 's04_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn3dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train 3d cnn and plot the loss curve\n",
    "plot_loss(cnn3dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 3d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn3dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "cnn3dtest_args = model_parser().parse_args(cnn3dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn3dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>AMICO</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writefile(path, file, scheme, savename):\n",
    "    movefile = open(scheme, 'r')\n",
    "    combine = np.array([int(num) for num in movefile.readline().split(' ')[:-1]])\n",
    "    with open(path+savename, 'w') as fout:\n",
    "        read_path = path+file\n",
    "        read_file = open(read_path, 'r')\n",
    "        lines = read_file.readlines()\n",
    "        for line in lines:\n",
    "            temp = line.split()\n",
    "            temp = [e for e, b in zip(temp, combine) if b == 1]\n",
    "            fout.write(' '.join(e for e in temp))\n",
    "            fout.write('\\n')\n",
    "    fout.close()\n",
    "\n",
    "def writediffusion(path, file, scheme):\n",
    "    movefile = open(scheme, 'r')\n",
    "    combine = np.array([int(num) for num in movefile.readline().split(' ')[:-1]])\n",
    "    img = nib.load(path+file)\n",
    "    data = img.get_fdata()\n",
    "    data = data[..., combine==1]\n",
    "    img = nib.Nifti1Image(data, np.eye(4))\n",
    "    nib.save(img, path+'remained_diffusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the remained bvals and bvecs for s03 by applying its corresponding rejection scheme\n",
    "s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/'\n",
    "# write the remained bvals\n",
    "writefile(s03_path, 'bvals', s03_movefile, savename='remained_bvals')\n",
    "# write the remained bvecs\n",
    "writefile(s03_path, 'bvecs', s03_movefile, savename='remained_bvecs')\n",
    "# create the remained diffusion data\n",
    "writediffusion(s03_path, 'diffusion.nii', s03_movefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amico.setup()\n",
    "# generate a scheme file from the bvals/bvecs files as follows, using the remained bvals and bvecs. Because data rejection is applied\n",
    "amico.util.fsl2scheme(s03_path+'remained_bvals', s03_path+'remained_bvecs')\n",
    "ae = amico.Evaluation(\"/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI\", \"s03_motion\")\n",
    "# load the data\n",
    "ae.load_data(dwi_filename = \"remained_diffusion.nii\", scheme_filename = \"remained_bvals.scheme\", mask_filename = \"mask-e.nii\", b0_thr = 0)\n",
    "# Set model for NODDI and generate the response functions for all the compartments:\n",
    "ae.set_model(\"NODDI\")\n",
    "ae.generate_kernels()\n",
    "ae.load_kernels()\n",
    "# model fit. It takes a little time depending on the number of voxels (but much much faster than the original NODDI).\n",
    "ae.fit()\n",
    "# Finally, save the results as NIfTI images:\n",
    "# ICVF = NDI\n",
    "# ISOVF = FWF\n",
    "# OD = ODI\n",
    "ae.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icvf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_ICVF.nii.gz'\n",
    "isovf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_ISOVF.nii.gz'\n",
    "od_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_OD.nii.gz'\n",
    "\n",
    "icvf_img = nib.load(icvf_path)\n",
    "icvf = icvf_img.get_fdata()\n",
    "isovf_img = nib.load(isovf_path)\n",
    "isovf = isovf_img.get_fdata()\n",
    "od_img = nib.load(od_path)\n",
    "od = od_img.get_fdata()\n",
    "\n",
    "icvf0 = icvf[26, :, :]\n",
    "icvf1 = icvf[:, 30, :]\n",
    "icvf2 = icvf[:, :, 16]\n",
    "show_slices([icvf0, icvf1, icvf2])\n",
    "plt.suptitle(\"Center slices for ICVF/NDI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(icvf, s03_NDI_img_data, full=True)\n",
    "print('the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "od0 = od[26, :, :]\n",
    "od1 = od[:, 30, :]\n",
    "od2 = od[:, :, 16]\n",
    "show_slices([od0, od1, od2])\n",
    "plt.suptitle(\"Center slices for OD/ODI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(od, s03_ODI_img_data, full=True)\n",
    "print('the ssim score for odi is: ' + str(score))\n",
    "\n",
    "isovf0 = isovf[26, :, :]\n",
    "isovf1 = isovf[:, 30, :]\n",
    "isovf2 = isovf[:, :, 16]\n",
    "show_slices([isovf0, isovf1, isovf2])\n",
    "plt.suptitle(\"Center slices for ISOVF/FWF predicted image by Amico\")\n",
    "(score, fwfdiff) = compare_ssim(isovf, s03_FWF_img_data, full=True)\n",
    "print('the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "icvfdiff0 = scale(s03_NDI_img_data[26, :, :]-icvf0)\n",
    "icvfdiff1 = scale(s03_NDI_img_data[:, 30, :]-icvf1)\n",
    "icvfdiff2 = scale(s03_NDI_img_data[:, :, 16]-icvf2)\n",
    "show_slices([icvfdiff0, icvfdiff1, icvfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ICVF/NDI diff map\")\n",
    "\n",
    "oddiff0 = scale(s03_ODI_img_data[26, :, :]-od0)\n",
    "oddiff1 = scale(s03_ODI_img_data[:, 30, :]-od1)\n",
    "oddiff2 = scale(s03_ODI_img_data[:, :, 16]-od2)\n",
    "show_slices([oddiff0, oddiff1, oddiff2], grayscale=False)\n",
    "plt.suptitle(\"OD/ODI diff map\")\n",
    "\n",
    "isovfdiff0 = scale(s03_FWF_img_data[26, :, :]-isovf0)\n",
    "isovfdiff1 = scale(s03_FWF_img_data[:, 30, :]-isovf1)\n",
    "isovfdiff2 = scale(s03_FWF_img_data[:, :, 16]-isovf2)\n",
    "show_slices([isovfdiff0, isovfdiff1, isovfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ISOVF/FWF diff map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "# create the remained bvals and bvecs for s04 by applying its corresponding rejection scheme\n",
    "s04_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/'\n",
    "# write the remained bvals\n",
    "writefile(s04_path, 'bvals', s04_movefile, savename='remained_bvals')\n",
    "# write the remained bvecs\n",
    "writefile(s04_path, 'bvecs', s04_movefile, savename='remained_bvecs')\n",
    "# create the remained diffusion data\n",
    "writediffusion(s04_path, 'diffusion.nii', s04_movefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amico.setup()\n",
    "# generate a scheme file from the bvals/bvecs files as follows, using the remained bvals and bvecs. Because data rejection is applied\n",
    "amico.util.fsl2scheme(s04_path+'remained_bvals', s04_path+'remained_bvecs')\n",
    "ae = amico.Evaluation(\"/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI\", \"s04_motion\")\n",
    "# load the data\n",
    "ae.load_data(dwi_filename = \"remained_diffusion.nii\", scheme_filename = \"remained_bvals.scheme\", mask_filename = \"mask-e.nii\", b0_thr = 0)\n",
    "# Set model for NODDI and generate the response functions for all the compartments:\n",
    "ae.set_model(\"NODDI\")\n",
    "ae.generate_kernels()\n",
    "ae.load_kernels()\n",
    "# model fit. It takes a little time depending on the number of voxels (but much much faster than the original NODDI).\n",
    "ae.fit()\n",
    "# Finally, save the results as NIfTI images:\n",
    "# ICVF = NDI\n",
    "# ISOVF = FWF\n",
    "# OD = ODI\n",
    "ae.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icvf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_ICVF.nii.gz'\n",
    "isovf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_ISOVF.nii.gz'\n",
    "od_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_OD.nii.gz'\n",
    "\n",
    "icvf_img = nib.load(icvf_path)\n",
    "icvf = icvf_img.get_fdata()\n",
    "isovf_img = nib.load(isovf_path)\n",
    "isovf = isovf_img.get_fdata()\n",
    "od_img = nib.load(od_path)\n",
    "od = od_img.get_fdata()\n",
    "\n",
    "icvf0 = icvf[26, :, :]\n",
    "icvf1 = icvf[:, 30, :]\n",
    "icvf2 = icvf[:, :, 16]\n",
    "show_slices([icvf0, icvf1, icvf2])\n",
    "plt.suptitle(\"Center slices for ICVF/NDI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(icvf, s04_NDI_img_data, full=True)\n",
    "print('the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "od0 = od[26, :, :]\n",
    "od1 = od[:, 30, :]\n",
    "od2 = od[:, :, 16]\n",
    "show_slices([od0, od1, od2])\n",
    "plt.suptitle(\"Center slices for OD/ODI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(od, s04_ODI_img_data, full=True)\n",
    "print('the ssim score for odi is: ' + str(score))\n",
    "\n",
    "isovf0 = isovf[26, :, :]\n",
    "isovf1 = isovf[:, 30, :]\n",
    "isovf2 = isovf[:, :, 16]\n",
    "show_slices([isovf0, isovf1, isovf2])\n",
    "plt.suptitle(\"Center slices for ISOVF/FWF predicted image by Amico\")\n",
    "(score, fwfdiff) = compare_ssim(isovf, s04_FWF_img_data, full=True)\n",
    "print('the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "icvfdiff0 = scale(s04_NDI_img_data[26, :, :]-icvf0)\n",
    "icvfdiff1 = scale(s04_NDI_img_data[:, 30, :]-icvf1)\n",
    "icvfdiff2 = scale(s04_NDI_img_data[:, :, 16]-icvf2)\n",
    "show_slices([icvfdiff0, icvfdiff1, icvfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ICVF/NDI diff map\")\n",
    "\n",
    "oddiff0 = scale(s04_ODI_img_data[26, :, :]-od0)\n",
    "oddiff1 = scale(s04_ODI_img_data[:, 30, :]-od1)\n",
    "oddiff2 = scale(s04_ODI_img_data[:, :, 16]-od2)\n",
    "show_slices([oddiff0, oddiff1, oddiff2], grayscale=False)\n",
    "plt.suptitle(\"OD/ODI diff map\")\n",
    "\n",
    "isovfdiff0 = scale(s04_FWF_img_data[26, :, :]-isovf0)\n",
    "isovfdiff1 = scale(s04_FWF_img_data[:, 30, :]-isovf1)\n",
    "isovfdiff2 = scale(s04_FWF_img_data[:, :, 16]-isovf2)\n",
    "show_slices([isovfdiff0, isovfdiff1, isovfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ISOVF/FWF diff map\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d00557ecee9d041f78bfa618225def395bc332f64d1935e44218a847c69687c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
