{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Experiment 2</h1>\n",
    "<h2>Test the performance of the proposed methodology on motion-corrupted data</h2>\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 21:58:06.000306: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "packages that does conventional model fitting\n",
    "\"\"\"\n",
    "import amico\n",
    "\"\"\"\n",
    "packages that generate train/test dataset\n",
    "\"\"\"\n",
    "from FormatData import generate_data, parser as data_parser\n",
    "\"\"\"\n",
    "packages that trains network\n",
    "\"\"\"\n",
    "from Training import train_network\n",
    "from utils.model import parser as model_parser\n",
    "\"\"\"\n",
    "packages that test network\n",
    "\"\"\"\n",
    "from Testing import test_model\n",
    "\"\"\"\n",
    "packages that produce the rejection shceme\n",
    "\"\"\"\n",
    "from filter_qa import parser as filter_parser, load_eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    \"\"\"\n",
    "    a class generate parser for cmd line args\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "packages that handle graphs\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from utils import calc_ssim\n",
    "%matplotlib inline\n",
    "def plot_loss(cmd):\n",
    "    \"\"\"\n",
    "    A function that used to plot the loss curve for the trained network.\n",
    "    Args:\n",
    "        cmd: String, the command line in the terminal\n",
    "    \"\"\"\n",
    "    args = model_parser().parse_args(cmd.split())\n",
    "    history = train_network(args)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def scale(img):\n",
    "    # for i in range(img.shape[0]):\n",
    "    #     for j in range(img.shape[1]):\n",
    "    #         img[i][j] = ((img[i][j]+1)/2)*255\n",
    "    return img\n",
    "\n",
    "def compare_simi(pred, ref):\n",
    "    return calc_ssim(pred, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(ref_ndi, ref_odi, ref_fwf, retained_vol, subject, model, layer, affine1, affine2, affine3):\n",
    "    \"\"\"\n",
    "    Function to visualise the imgs and difference maps\n",
    "\n",
    "    Args:\n",
    "        ref_ndi (ndarray): the reference NDI data\n",
    "        ref_odi (ndarray): the reference ODI data\n",
    "        ref_fwf (ndarray): the reference FWF data\n",
    "        retained_vol (int): the number of volumes retained after rejection\n",
    "        subject (string): the subject that is examined\n",
    "        model (string): the model used\n",
    "        layer (int): the number of layers for the network\n",
    "    \"\"\"\n",
    "    patch = 3\n",
    "    if model == 'fc1d':\n",
    "        patch = 1\n",
    "\n",
    "    print(patch)\n",
    "    print(model)\n",
    "    # visualise the ref imgs\n",
    "    refNDI0 = ref_ndi[26, :, :]\n",
    "    refNDI1 = ref_ndi[:, 30, :]\n",
    "    refNDI2 = ref_ndi[:, :, 16]\n",
    "    show_slices([refNDI0, refNDI1, refNDI2])\n",
    "    plt.suptitle(\"Center slices for NDI reference image\")\n",
    "\n",
    "    refODI0 = ref_odi[26, :, :]\n",
    "    refODI1 = ref_odi[:, 30, :]\n",
    "    refODI2 = ref_odi[:, :, 16]\n",
    "    show_slices([refODI0, refODI1, refODI2])\n",
    "    plt.suptitle(\"Center slices for ODI reference image\")\n",
    "\n",
    "    refFWF0 = ref_fwf[26, :, :]\n",
    "    refFWF1 = ref_fwf[:, 30, :]\n",
    "    refFWF2 = ref_fwf[:, :, 16]\n",
    "    show_slices([refFWF0, refFWF1, refFWF2])\n",
    "    plt.suptitle(\"Center slices for FWF reference image\")\n",
    "\n",
    "    # visualise the pred imgs produced at varied input size\n",
    "    ndi_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_NDI.nii'\n",
    "    odi_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_ODI.nii'\n",
    "    fwf_path = '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_FWF.nii'\n",
    "    ndi_img = nib.load(ndi_path)\n",
    "    ndi_data = ndi_img.get_fdata()\n",
    "    odi_img = nib.load(odi_path)\n",
    "    odi_data = odi_img.get_fdata()\n",
    "    fwf_img = nib.load(fwf_path)\n",
    "    fwf_data = fwf_img.get_fdata()\n",
    "\n",
    "    ndi0 = ndi_data[26, :, :]\n",
    "    ndi1 = ndi_data[:, 30, :]\n",
    "    ndi2 = ndi_data[:, :, 16]\n",
    "    show_slices([ndi0, ndi1, ndi2])\n",
    "    plt.suptitle('Center slices for NDI predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, ndidiff) = compare_ssim(ndi_data, ref_ndi, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "    odi0 = odi_data[26, :, :]\n",
    "    odi1 = odi_data[:, 30, :]\n",
    "    odi2 = odi_data[:, :, 16]\n",
    "    show_slices([odi0, odi1, odi2])\n",
    "    plt.suptitle('Center slices for ODI predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, odidiff) = compare_ssim(odi_data, ref_odi, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for odi is: ' + str(score))\n",
    "\n",
    "    fwf0 = fwf_data[26, :, :]\n",
    "    fwf1 = fwf_data[:, 30, :]\n",
    "    fwf2 = fwf_data[:, :, 16]\n",
    "    show_slices([fwf0, fwf1, fwf2])\n",
    "    plt.suptitle('Center slices for FWF predicted image by '+model+', input size='+str(retained_vol))\n",
    "    (score, fwfdiff) = compare_ssim(fwf_data, ref_fwf, full=True)\n",
    "    print(str(retained_vol)+'input size the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "    # plot the difference map between the imgs by the lib\n",
    "    ndidiff0 = scale(refNDI0 - ndi0)\n",
    "    ndidiff1 = scale(refNDI1 - ndi1)\n",
    "    ndidiff2 = scale(refNDI2 - ndi2)\n",
    "    show_slices([ndidiff0, ndidiff1, ndidiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map NDI\")\n",
    "\n",
    "    diff_img_np = ref_ndi - ndi_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine1)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_NDI_difference.nii')\n",
    "\n",
    "    odidiff0 = scale(refODI0 - odi0)\n",
    "    odidiff1 = scale(refODI1 - odi1)\n",
    "    odidiff2 = scale(refODI2 - odi2)\n",
    "    show_slices([odidiff0, odidiff1, odidiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map ODI\")\n",
    "\n",
    "    diff_img_np = ref_odi - odi_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine2)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_ODI_difference.nii')\n",
    "\n",
    "    fwfdiff0 = scale(refFWF0 - fwf0)\n",
    "    fwfdiff1 = scale(refFWF1 - fwf1)\n",
    "    fwfdiff2 = scale(refFWF2 - fwf2)\n",
    "    show_slices([fwfdiff0, fwfdiff1, fwfdiff2], grayscale=False)\n",
    "    plt.suptitle(\"Difference map FWF\")\n",
    "\n",
    "    diff_img_np = ref_fwf - fwf_data\n",
    "    diff_img = nib.Nifti1Image(diff_img_np, affine3)\n",
    "    nib.save(diff_img, '../Net/nii/'+subject+'-'+str(retained_vol)+'-'+model+'-patch_'+str(patch)+'-base_1-layer_'+str(layer)+'-label_FWF_difference.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion-free subject path\n",
    "# s01_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/'\n",
    "# s02_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/'\n",
    "# s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/'\n",
    "s04_path = '/home/vw/Data/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/'\n",
    "# motion-free target labels\n",
    "# s01_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_NDI.nii'\n",
    "# s02_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_NDI.nii'\n",
    "# s03_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_NDI.nii'\n",
    "# s04_NDI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_NDI.nii'\n",
    "\n",
    "# s01_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_ODI.nii'\n",
    "# s02_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_ODI.nii'\n",
    "# s03_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_ODI.nii'\n",
    "# s04_ODI_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_ODI.nii'\n",
    "\n",
    "# s01_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/s01_still_FWF.nii'\n",
    "# s02_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/s02_still_FWF.nii'\n",
    "# s03_FWF_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/s03_still_reg_FWF.nii'\n",
    "s04_FWF_path = '/home/vw/Data/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/s04_still_reg_FWF.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mask(subpath, fwfpath, threshold=0.99):\n",
    "    \"\"\"\n",
    "    By looking at the imgs generated, we have found out there are some regions that should not be included. Since they have values higher than 1.0\n",
    "    And we have found out voxels have NDI and ODI values, while that voxel has GROUND TRUTH FWF 1.0\n",
    "    This should indicate that that voxel should not even be included in the training\n",
    "    Therefore we want to filter the each subject's mask first, by using their corresponding GROUND TRUTH FWF\n",
    "\n",
    "    Args:\n",
    "        subpath (string): the path of the subject folder\n",
    "        fwfpath (string): the path of the corresponding fwf file\n",
    "        threshold (float): the thresholds to be used to filter of the mask,\n",
    "                           a stringnent threshold would be 0.9, the least stringnent threshold is 1.0\n",
    "                           by default, it is set to 0.99\n",
    "    \"\"\"\n",
    "    # fetch the mask data\n",
    "    img_mask = nib.load(subpath+'mask-e.nii')\n",
    "    original_mask = img_mask.get_fdata()\n",
    "    original_affine = img_mask.affine\n",
    "    shape = original_mask.shape # retain the shape of the mask\n",
    "    origin_nonzeros = np.count_nonzero(original_mask)\n",
    "    print('original mask has: ' + str(origin_nonzeros) + ' of nonzero voxels')\n",
    "    # fetch the FWF data\n",
    "    fwf = nib.load(fwfpath).get_fdata()\n",
    "    # filter\n",
    "    mask = original_mask.flatten() # this makes a copy of the orginal mask\n",
    "    fwf = fwf.reshape(mask.shape[0]) # reshape fwf to the corresponding shape\n",
    "    for i in range(len(mask)):\n",
    "        # if fwf has high value, means there is no tissue\n",
    "        # therefore, the voxel should be excluded\n",
    "        if fwf[i] >= threshold:\n",
    "            mask[i] = 0.0\n",
    "    # reshape mask back\n",
    "    mask = mask.reshape(shape)\n",
    "    filter_nonzeros = np.count_nonzero(mask)\n",
    "    print('filtered mask has: ' +str(filter_nonzeros) + ' of nonzero voxels')\n",
    "    # save the mask\n",
    "    filter_img = nib.Nifti1Image(mask, original_affine)\n",
    "    nib.save(filter_img, subpath+'filtered_mask.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mask has: 88241 of nonzero voxels\n",
      "filtered mask has: 86494 of nonzero voxels\n"
     ]
    }
   ],
   "source": [
    "# Use the above code to filter each subject's mask. Store as filtered_mask.nii in each subject folder\n",
    "# filter_mask(s01_path, s01_FWF_path)\n",
    "# filter_mask(s02_path, s02_FWF_path)\n",
    "# filter_mask(s03_path, s03_FWF_path)\n",
    "filter_mask(s04_path, s04_FWF_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered mask path for each subject\n",
    "# s01_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s01_still/filtered_mask.nii'\n",
    "# s02_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s02_still/filtered_mask.nii'\n",
    "# s03_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_still_reg/filtered_mask.nii'\n",
    "s04_mask_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_still_reg/filtered_mask.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for s04_still_reg ...\n",
      "(88241, 96)\n",
      "(84, 84, 50)\n",
      "base data dataset has shape: (84, 84, 50, 96)\n",
      "base label dataset has shape: (84, 84, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate the base dataset for s01_still, s02_still, s03_still_reg and s04_still_reg first for all NODDI parameters.\n",
    "\"\"\"\n",
    "cmd = \"--base --label_type A --subjects s04_still_reg --path /home/vw/Data/IndividualProject/MedICSS2021_/Data-NODDI\"\n",
    "args = data_parser().parse_args(cmd.split())\n",
    "generate_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using nib to fetch the ground truth img\n",
    "\"\"\"\n",
    "#load the truth data for subject 1\n",
    "s01_NDI_img = nib.load(s01_NDI_path)\n",
    "s01_ODI_img = nib.load(s01_ODI_path)\n",
    "s01_FWF_img = nib.load(s01_FWF_path)\n",
    "s01_NDI_affine = s01_NDI_img.affine\n",
    "s01_ODI_affine = s01_ODI_img.affine\n",
    "s01_FWF_affine = s01_FWF_img.affine\n",
    "s01_NDI_img_data = s01_NDI_img.get_fdata()\n",
    "s01_ODI_img_data = s01_ODI_img.get_fdata()\n",
    "s01_FWF_img_data = s01_FWF_img.get_fdata()\n",
    "#load the truth data for subject 2\n",
    "s02_NDI_img = nib.load(s02_NDI_path)\n",
    "s02_ODI_img = nib.load(s02_ODI_path)\n",
    "s02_FWF_img = nib.load(s02_FWF_path)\n",
    "s02_NDI_affine = s02_NDI_img.affine\n",
    "s02_ODI_affine = s02_ODI_img.affine\n",
    "s02_FWF_affine = s02_FWF_img.affine\n",
    "s02_NDI_img_data = s02_NDI_img.get_fdata()\n",
    "s02_ODI_img_data = s02_ODI_img.get_fdata()\n",
    "s02_FWF_img_data = s02_FWF_img.get_fdata()\n",
    "# load the truth data for subject 3\n",
    "s03_NDI_img = nib.load(s03_NDI_path)\n",
    "s03_ODI_img = nib.load(s03_ODI_path)\n",
    "s03_FWF_img = nib.load(s03_FWF_path)\n",
    "s03_NDI_affine = s03_NDI_img.affine\n",
    "s03_ODI_affine = s03_ODI_img.affine\n",
    "s03_FWF_affine = s03_FWF_img.affine\n",
    "s03_NDI_img_data = s03_NDI_img.get_fdata()\n",
    "s03_ODI_img_data = s03_ODI_img.get_fdata()\n",
    "s03_FWF_img_data = s03_FWF_img.get_fdata()\n",
    "# load the truth data for subject 4\n",
    "s04_NDI_img = nib.load(s04_NDI_path)\n",
    "s04_ODI_img = nib.load(s04_ODI_path)\n",
    "s04_FWF_img = nib.load(s04_FWF_path)\n",
    "s04_NDI_affine = s04_NDI_img.affine\n",
    "s04_ODI_affine = s04_ODI_img.affine\n",
    "s04_FWF_affine = s04_FWF_img.affine\n",
    "s04_NDI_img_data = s04_NDI_img.get_fdata()\n",
    "s04_ODI_img_data = s04_ODI_img.get_fdata()\n",
    "s04_FWF_img_data = s04_FWF_img.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the proposed networks, 4 layer ANN, 2D CNN and 3D CNN, to carry out this experiment. The networks are required to be trained again, since this time we will use the rejection scheme to select the input DWI rather than using the sequential scheme.<br/>\n",
    "To use the rejection scheme, we are firstly required to use filter_qa.py to set up the thresholds to obtain the retained volumes.<br/>\n",
    "<h4>Filtering the data</h4><br/>\n",
    "To filter the data, user needs to provide the path for s03_motion and s04_motion, by that filter_qa can access the QAfrom-eddylog.txt. Using the thresholds set by the users, the code could produce a rejection scheme file that has binary values in it. Where 1 represents the one has motion-level below the thresholds; whereas 0 represents the one has higher motion-level than the thresholds. Hence, when the rejection scheme is applied to the (training/testing) datasets, the code would select the volumes having corresponding 1s in the scheme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the motion data\n",
    "s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion' # the path of subject 3 motion\n",
    "s04_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion' # the path of subject 3 motion\n",
    "# set the thresholds, and the number of DWIs to be examined. THe thresholds are all float numbers\n",
    "# stringnet thresholds are used t0=Absolute Translation, t1=Relative Translation, r0=Absolute Rotation, r1=Relative Rotation and out=Fractional Signal Dropouts\n",
    "t0, t1, r0, r1, out, num = 3.0, 2.0, 3.0, 2.0, 0.05, 96 \n",
    "# produce the filter cmd and args\n",
    "filter_cmd = '--path ' + s03_path + ' --t0 ' + str(t0) + ' --t1 ' + str(t1) + ' --r0 ' + str(r0) + ' --r1 ' + str(r1) + ' --out ' + str(out) + ' --num ' + str(num)\n",
    "filter_args = filter_parser().parse_args(filter_cmd.split())\n",
    "# this should generate move_t0-{t0}_t1-{t1}_r0-{r0}_r1-{r1}_out-{out}.txt file in the path of subject 3 motion for s03 first\n",
    "load_eddy(filter_args)\n",
    "\n",
    "filter_cmd = '--path ' + s04_path + ' --t0 ' + str(t0) + ' --t1 ' + str(t1) + ' --r0 ' + str(r0) + ' --r1 ' + str(r1) + ' --out ' + str(out) + ' --num ' + str(num)\n",
    "filter_args = filter_parser().parse_args(filter_cmd.split())\n",
    "# rejection scheme for s04 is generated\n",
    "load_eddy(filter_args)\n",
    "\n",
    "# the path of the rejection schemes\n",
    "s03_movefile = s03_path + '/move_t0-'+str(t0)+'_t1-'+str(t1)+'_r0-'+str(r0)+'_r1-'+str(r1)+'_out-'+str(out)+'.txt'\n",
    "s04_movefile = s04_path + '/move_t0-'+str(t0)+'_t1-'+str(t1)+'_r0-'+str(r0)+'_r1-'+str(r1)+'_out-'+str(out)+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training</h4><br/>\n",
    "Now, we will use the rejection scheme we have, to fetch the training data for training our proposed networks.<br/>\n",
    "Firstly, we will define the hyperparameters that will be used for all proposed networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the networks\n",
    "layers = 5 # the number of hidden layers; this is the optimal number of layer\n",
    "lr = 0.0001 # the learning rate\n",
    "patch_size = 3 # the size of patches for 2D and 3D CNN\n",
    "batch = 256 # the batch size\n",
    "epoch = 100 # the number of epoches for training\n",
    "ltype = 'A' # the NODDI parameter we want to estimate, where A stands for ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>ANN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'fc1d'\n",
    "# firstly, the ann dataset is required to be generated.\n",
    "# the cmd line code for generating ANN data\n",
    "anndata_cmd = '--subjects s01_still s02_still s03_still_reg s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "anndata_args  = data_parser().parse_args(anndata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-1d-1-1-all.mat ... s04_still_reg-base1-patches-1d-1-1-all.mat under Net/datasets/data\n",
    "generate_data(anndata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "anntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s03_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(anntrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained ANN by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "anntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s03_movefile\n",
    "anntest_args = model_parser().parse_args(anntest_cmd.split())\n",
    "# test\n",
    "test_model(anntest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers,affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "study_subject = 's04_still_reg' # this is the name of the study subject\n",
    "sep_train_subject = 's01_still' # this is the name of the separate training candidate\n",
    "# we use the rejection scheme produced from the study subject to select the desired subset from the separate training dataset. And we train on the separate training dataset\n",
    "anntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(anntrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained ANN by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "anntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "anntest_args = model_parser().parse_args(anntest_cmd.split())\n",
    "# test\n",
    "test_model(anntest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>2D CNN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'conv2d'\n",
    "# firstly, the 2d cnn dataset is required to be generated.\n",
    "# the cmd line code for generating 2d cnn data\n",
    "cnn2ddata_cmd = '--subjects s01_still s02_still s03_still_reg s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "cnn2ddata_args  = data_parser().parse_args(cnn2ddata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-2d-3-1-all.mat ... s04_still_reg-base1-patches-3d-3-1-all.mat under Net/datasets/data\n",
    "generate_data(cnn2ddata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn2dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s03_movefile + ' --train'\n",
    "# train 2d cnn and plot the loss curve\n",
    "plot_loss(cnn2dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 2d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn2dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s03_movefile\n",
    "cnn2dtest_args = model_parser().parse_args(cnn2dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn2dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process for study subject s04\n",
    "# we study subject s04, by using s01 as a seperate training dataset\n",
    "study_subject = 's04_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn2dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train ANN and plot the loss curve\n",
    "plot_loss(cnn2dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 2d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn2dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "cnn2dtest_args = model_parser().parse_args(cnn2dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn2dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>3D CNN</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for s04_still_reg ...\n",
      "mask has shape: (82, 82, 48)\n",
      "data has shape: (84, 84, 50, 96)\n",
      "(84, 84, 50, 96) (82, 82, 48)\n",
      "(86056, 3, 3, 3, 96)\n",
      "saved patches has shape: (86056, 2592)\n",
      "label has shape: (82, 82, 48, 3)\n",
      "(82, 82, 48, 3) (82, 82, 48)\n",
      "(86056, 1, 1, 1, 3)\n",
      "svaed labels has shape: (86056, 1, 1, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "model = 'conv3d'\n",
    "# firstly, the 3d cnn dataset is required to be generated.\n",
    "# the cmd line code for generating 3d cnn data\n",
    "cnn3ddata_cmd = '--subjects s04_still_reg' + ' --label_type ' + ltype + ' --' + model\n",
    "cnn3ddata_args  = data_parser().parse_args(cnn3ddata_cmd.split())\n",
    "# this should produce s01_still-base1-patches-3d-3-1-all.mat ... s04_still_reg-base1-patches-3d-3-1-all.mat under Net/datasets/data\n",
    "generate_data(cnn3ddata_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s01_still conv3d 5 A 0.0001 256 3 100\n",
      "96\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3, 3, 3, 96)]     0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 1, 1, 1, 150)      388950    \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 1, 1, 1, 150)      22650     \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 1, 1, 1, 150)      22650     \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 1, 1, 1, 150)      22650     \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 1, 1, 1, 150)      22650     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1, 150)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 1, 1, 1, 3)        453       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1, 1, 1, 3)        0         \n",
      "=================================================================\n",
      "Total params: 480,003\n",
      "Trainable params: 480,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 22:08:50.663355: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-08 22:08:50.684512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-08 22:08:50.684595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2022-07-08 22:08:50.684614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-08 22:08:50.685474: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-08 22:08:50.701596: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-08 22:08:50.701899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-08 22:08:50.702717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-08 22:08:50.703086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-08 22:08:50.703184: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2022-07-08 22:08:50.703188: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-08 22:08:50.703466: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-08 22:08:50.729810: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2496000000 Hz\n",
      "2022-07-08 22:08:50.730374: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562f26388710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-08 22:08:50.730387: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-07-08 22:08:50.731392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-08 22:08:50.731399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remained data has shape: (81634, 3, 3, 3, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 22:08:54.043638: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1692762624 exceeds 10% of free system memory.\n",
      "2022-07-08 22:08:54.562562: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 846381312 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81634, 3, 3, 3, 96)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 150)\n",
      "(81634, 1, 1, 1, 3)\n",
      "(81634, 1, 1, 1, 3)\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 22:08:54.862921: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2022-07-08 22:08:54.862964: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\n",
      "2022-07-08 22:08:54.864340: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.1\n",
      "2022-07-08 22:08:54.864962: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vw/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 22:08:55.405193: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "WARNING:tensorflow:From /home/vw/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 0.0098s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 22:08:55.412546: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\n",
      "2022-07-08 22:08:55.412944: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-07-08 22:08:55.415758: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/train/plugins/profile/2022_07_08_22_08_55\n",
      "2022-07-08 22:08:55.416378: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/train/plugins/profile/2022_07_08_22_08_55/vw.trace.json.gz\n",
      "2022-07-08 22:08:55.416457: E tensorflow/core/profiler/utils/hardware_type_utils.cc:61] Invalid GPU compute capability.\n",
      "2022-07-08 22:08:55.419208: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/train/plugins/profile/2022_07_08_22_08_55\n",
      "2022-07-08 22:08:55.419260: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/train/plugins/profile/2022_07_08_22_08_55/vw.memory_profile.json.gz\n",
      "2022-07-08 22:08:55.420934: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/train/plugins/profile/2022_07_08_22_08_55Dumped tool data for xplane.pb to logs/train/plugins/profile/2022_07_08_22_08_55/vw.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/train/plugins/profile/2022_07_08_22_08_55/vw.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/train/plugins/profile/2022_07_08_22_08_55/vw.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/train/plugins/profile/2022_07_08_22_08_55/vw.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/train/plugins/profile/2022_07_08_22_08_55/vw.kernel_stats.pb\n",
      "\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 0.0098s). Check your callbacks.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we firstly study subject s03, by using s01 as a seperate training dataset\n",
    "study_subject = 's04_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "print(sep_train_subject, model, layers, ltype, lr, batch, patch_size, epoch)\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn3dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --train'\n",
    "# train 3d cnn and plot the loss curve\n",
    "plot_loss(cnn3dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96-conv3d-patch_3-base_1-layer_5-label_NDIODIFWFsynthetic\n",
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 84, 84, 50, 96)]  0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 82, 82, 48, 150)   388950    \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 82, 82, 48, 150)   22650     \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 82, 82, 48, 150)   22650     \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 82, 82, 48, 150)   22650     \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 82, 82, 48, 150)   22650     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 82, 82, 48, 150)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 82, 82, 48, 3)     453       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 82, 82, 48, 3)     0         \n",
      "=================================================================\n",
      "Total params: 480,003\n",
      "Trainable params: 480,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "prediction after repack has shape: (84, 84, 50, 3)\n",
      "pred ndi has shape: (84, 84, 50)\n",
      "pred shape: (86056,)\n",
      "gt has shape: (86056,)\n",
      "0.09702799083785471\n",
      "pred shape: (86056,)\n",
      "gt has shape: (86056,)\n",
      "pred shape: (86056,)\n",
      "gt has shape: (86056,)\n",
      "the RMSE loss is: [0.09702799083785471, 0.06799634743748809, 0.05622500890720967]\n",
      "the SSIM loss is: [-0.094, 0.129, 0.936]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.09702799083785471, 0.06799634743748809, 0.05622500890720967],\n",
       " [-0.094, 0.129, 0.936])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have trained 3d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn3dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype\n",
    "cnn3dtest_args = model_parser().parse_args(cnn3dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn3dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 29 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s03_NDI_img_data, s03_ODI_img_data, s03_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s03_NDI_affine, affine2=s03_ODI_affine, affine3=s03_FWF_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "# we study subject s04, by using s01 as a seperate training dataset\n",
    "study_subject = 's04_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate\n",
    "# the cmd line code for training, we will apply the rejection scheme from study_subject to the separate training dataset\n",
    "cnn3dtrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + s04_movefile + ' --train'\n",
    "# train 3d cnn and plot the loss curve\n",
    "plot_loss(cnn3dtrain_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have trained 3d cnn by using a desired subset of a separate training dataset. We would like to apply this model onto the remained volumes of study subject\n",
    "# the cmd line code for testing\n",
    "cnn3dtest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + s04_movefile\n",
    "cnn3dtest_args = model_parser().parse_args(cnn3dtest_cmd.split())\n",
    "# test\n",
    "test_model(cnn3dtest_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the predicted NODDI parameters\n",
    "vol = 10 # the number of volumes remained after rejection scheme is applied\n",
    "visualise(s04_NDI_img_data, s04_ODI_img_data, s04_FWF_img_data, retained_vol=vol,subject=study_subject, model=model, layer=layers, affine1=s04_NDI_affine, affine2=s04_ODI_affine, affine3=s04_FWF_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<strong>AMICO</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writefile(path, file, scheme, savename):\n",
    "    movefile = open(scheme, 'r')\n",
    "    combine = np.array([int(num) for num in movefile.readline().split(' ')[:-1]])\n",
    "    with open(path+savename, 'w') as fout:\n",
    "        read_path = path+file\n",
    "        read_file = open(read_path, 'r')\n",
    "        lines = read_file.readlines()\n",
    "        for line in lines:\n",
    "            temp = line.split()\n",
    "            temp = [e for e, b in zip(temp, combine) if b == 1]\n",
    "            fout.write(' '.join(e for e in temp))\n",
    "            fout.write('\\n')\n",
    "    fout.close()\n",
    "\n",
    "def writediffusion(path, file, scheme):\n",
    "    movefile = open(scheme, 'r')\n",
    "    combine = np.array([int(num) for num in movefile.readline().split(' ')[:-1]])\n",
    "    img = nib.load(path+file)\n",
    "    data = img.get_fdata()\n",
    "    data = data[..., combine==1]\n",
    "    img = nib.Nifti1Image(data, np.eye(4))\n",
    "    nib.save(img, path+'remained_diffusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the remained bvals and bvecs for s03 by applying its corresponding rejection scheme\n",
    "s03_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/'\n",
    "# write the remained bvals\n",
    "writefile(s03_path, 'bvals', s03_movefile, savename='remained_bvals')\n",
    "# write the remained bvecs\n",
    "writefile(s03_path, 'bvecs', s03_movefile, savename='remained_bvecs')\n",
    "# create the remained diffusion data\n",
    "writediffusion(s03_path, 'diffusion.nii', s03_movefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amico.setup()\n",
    "# generate a scheme file from the bvals/bvecs files as follows, using the remained bvals and bvecs. Because data rejection is applied\n",
    "amico.util.fsl2scheme(s03_path+'remained_bvals', s03_path+'remained_bvecs')\n",
    "ae = amico.Evaluation(\"/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI\", \"s03_motion\")\n",
    "# load the data\n",
    "ae.load_data(dwi_filename = \"remained_diffusion.nii\", scheme_filename = \"remained_bvals.scheme\", mask_filename = \"mask-e.nii\", b0_thr = 0)\n",
    "# Set model for NODDI and generate the response functions for all the compartments:\n",
    "ae.set_model(\"NODDI\")\n",
    "ae.generate_kernels()\n",
    "ae.load_kernels()\n",
    "# model fit. It takes a little time depending on the number of voxels (but much much faster than the original NODDI).\n",
    "ae.fit()\n",
    "# Finally, save the results as NIfTI images:\n",
    "# ICVF = NDI\n",
    "# ISOVF = FWF\n",
    "# OD = ODI\n",
    "ae.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icvf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_ICVF.nii.gz'\n",
    "isovf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_ISOVF.nii.gz'\n",
    "od_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s03_motion/AMICO/NODDI/FIT_OD.nii.gz'\n",
    "\n",
    "icvf_img = nib.load(icvf_path)\n",
    "icvf = icvf_img.get_fdata()\n",
    "isovf_img = nib.load(isovf_path)\n",
    "isovf = isovf_img.get_fdata()\n",
    "od_img = nib.load(od_path)\n",
    "od = od_img.get_fdata()\n",
    "\n",
    "icvf0 = icvf[26, :, :]\n",
    "icvf1 = icvf[:, 30, :]\n",
    "icvf2 = icvf[:, :, 16]\n",
    "show_slices([icvf0, icvf1, icvf2])\n",
    "plt.suptitle(\"Center slices for ICVF/NDI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(icvf, s03_NDI_img_data, full=True)\n",
    "print('the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "od0 = od[26, :, :]\n",
    "od1 = od[:, 30, :]\n",
    "od2 = od[:, :, 16]\n",
    "show_slices([od0, od1, od2])\n",
    "plt.suptitle(\"Center slices for OD/ODI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(od, s03_ODI_img_data, full=True)\n",
    "print('the ssim score for odi is: ' + str(score))\n",
    "\n",
    "isovf0 = isovf[26, :, :]\n",
    "isovf1 = isovf[:, 30, :]\n",
    "isovf2 = isovf[:, :, 16]\n",
    "show_slices([isovf0, isovf1, isovf2])\n",
    "plt.suptitle(\"Center slices for ISOVF/FWF predicted image by Amico\")\n",
    "(score, fwfdiff) = compare_ssim(isovf, s03_FWF_img_data, full=True)\n",
    "print('the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "icvfdiff0 = scale(s03_NDI_img_data[26, :, :]-icvf0)\n",
    "icvfdiff1 = scale(s03_NDI_img_data[:, 30, :]-icvf1)\n",
    "icvfdiff2 = scale(s03_NDI_img_data[:, :, 16]-icvf2)\n",
    "show_slices([icvfdiff0, icvfdiff1, icvfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ICVF/NDI diff map\")\n",
    "\n",
    "oddiff0 = scale(s03_ODI_img_data[26, :, :]-od0)\n",
    "oddiff1 = scale(s03_ODI_img_data[:, 30, :]-od1)\n",
    "oddiff2 = scale(s03_ODI_img_data[:, :, 16]-od2)\n",
    "show_slices([oddiff0, oddiff1, oddiff2], grayscale=False)\n",
    "plt.suptitle(\"OD/ODI diff map\")\n",
    "\n",
    "isovfdiff0 = scale(s03_FWF_img_data[26, :, :]-isovf0)\n",
    "isovfdiff1 = scale(s03_FWF_img_data[:, 30, :]-isovf1)\n",
    "isovfdiff2 = scale(s03_FWF_img_data[:, :, 16]-isovf2)\n",
    "show_slices([isovfdiff0, isovfdiff1, isovfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ISOVF/FWF diff map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same process for subject 4\n",
    "# create the remained bvals and bvecs for s04 by applying its corresponding rejection scheme\n",
    "s04_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/'\n",
    "# write the remained bvals\n",
    "writefile(s04_path, 'bvals', s04_movefile, savename='remained_bvals')\n",
    "# write the remained bvecs\n",
    "writefile(s04_path, 'bvecs', s04_movefile, savename='remained_bvecs')\n",
    "# create the remained diffusion data\n",
    "writediffusion(s04_path, 'diffusion.nii', s04_movefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amico.setup()\n",
    "# generate a scheme file from the bvals/bvecs files as follows, using the remained bvals and bvecs. Because data rejection is applied\n",
    "amico.util.fsl2scheme(s04_path+'remained_bvals', s04_path+'remained_bvecs')\n",
    "ae = amico.Evaluation(\"/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI\", \"s04_motion\")\n",
    "# load the data\n",
    "ae.load_data(dwi_filename = \"remained_diffusion.nii\", scheme_filename = \"remained_bvals.scheme\", mask_filename = \"mask-e.nii\", b0_thr = 0)\n",
    "# Set model for NODDI and generate the response functions for all the compartments:\n",
    "ae.set_model(\"NODDI\")\n",
    "ae.generate_kernels()\n",
    "ae.load_kernels()\n",
    "# model fit. It takes a little time depending on the number of voxels (but much much faster than the original NODDI).\n",
    "ae.fit()\n",
    "# Finally, save the results as NIfTI images:\n",
    "# ICVF = NDI\n",
    "# ISOVF = FWF\n",
    "# OD = ODI\n",
    "ae.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icvf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_ICVF.nii.gz'\n",
    "isovf_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_ISOVF.nii.gz'\n",
    "od_path = '/home/vw/Desktop/IndividualProject/MedICSS2021_/Data-NODDI/s04_motion/AMICO/NODDI/FIT_OD.nii.gz'\n",
    "\n",
    "icvf_img = nib.load(icvf_path)\n",
    "icvf = icvf_img.get_fdata()\n",
    "isovf_img = nib.load(isovf_path)\n",
    "isovf = isovf_img.get_fdata()\n",
    "od_img = nib.load(od_path)\n",
    "od = od_img.get_fdata()\n",
    "\n",
    "icvf0 = icvf[26, :, :]\n",
    "icvf1 = icvf[:, 30, :]\n",
    "icvf2 = icvf[:, :, 16]\n",
    "show_slices([icvf0, icvf1, icvf2])\n",
    "plt.suptitle(\"Center slices for ICVF/NDI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(icvf, s04_NDI_img_data, full=True)\n",
    "print('the ssim score for ndi is: ' + str(score))\n",
    "\n",
    "od0 = od[26, :, :]\n",
    "od1 = od[:, 30, :]\n",
    "od2 = od[:, :, 16]\n",
    "show_slices([od0, od1, od2])\n",
    "plt.suptitle(\"Center slices for OD/ODI predicted image by Amico\")\n",
    "(score, ndidiff) = compare_ssim(od, s04_ODI_img_data, full=True)\n",
    "print('the ssim score for odi is: ' + str(score))\n",
    "\n",
    "isovf0 = isovf[26, :, :]\n",
    "isovf1 = isovf[:, 30, :]\n",
    "isovf2 = isovf[:, :, 16]\n",
    "show_slices([isovf0, isovf1, isovf2])\n",
    "plt.suptitle(\"Center slices for ISOVF/FWF predicted image by Amico\")\n",
    "(score, fwfdiff) = compare_ssim(isovf, s04_FWF_img_data, full=True)\n",
    "print('the ssim score for fwf is: ' + str(score))\n",
    "\n",
    "icvfdiff0 = scale(s04_NDI_img_data[26, :, :]-icvf0)\n",
    "icvfdiff1 = scale(s04_NDI_img_data[:, 30, :]-icvf1)\n",
    "icvfdiff2 = scale(s04_NDI_img_data[:, :, 16]-icvf2)\n",
    "show_slices([icvfdiff0, icvfdiff1, icvfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ICVF/NDI diff map\")\n",
    "\n",
    "oddiff0 = scale(s04_ODI_img_data[26, :, :]-od0)\n",
    "oddiff1 = scale(s04_ODI_img_data[:, 30, :]-od1)\n",
    "oddiff2 = scale(s04_ODI_img_data[:, :, 16]-od2)\n",
    "show_slices([oddiff0, oddiff1, oddiff2], grayscale=False)\n",
    "plt.suptitle(\"OD/ODI diff map\")\n",
    "\n",
    "isovfdiff0 = scale(s04_FWF_img_data[26, :, :]-isovf0)\n",
    "isovfdiff1 = scale(s04_FWF_img_data[:, 30, :]-isovf1)\n",
    "isovfdiff2 = scale(s04_FWF_img_data[:, :, 16]-isovf2)\n",
    "show_slices([isovfdiff0, isovfdiff1, isovfdiff2], grayscale=False)\n",
    "plt.suptitle(\"ISOVF/FWF diff map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d00557ecee9d041f78bfa618225def395bc332f64d1935e44218a847c69687c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
