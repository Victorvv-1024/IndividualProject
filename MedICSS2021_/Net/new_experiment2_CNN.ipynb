{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>New Experiment 2: Random rejection of motion-free subjects</h1>\n",
    "In this notebook, random rejection tests were conducted using data from S1 in the still condition. The generalisation tests were conducted over S2. Each time we fix the number of retained volumes N. We randomly select N volumes from a subject's data, then we train our proposed model over this sample. At testing, we applied the same rejection scheme to S2, then test over the subset of S2.</br>\n",
    "We would like to show this proposed model is robust, by which it is independent of 1. The proportion of discarded volumes and 2. The different combinations of b-values.</br>\n",
    "<h2>Packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\"\"\"\n",
    "packages that does conventional model fitting\n",
    "\"\"\"\n",
    "import amico\n",
    "\"\"\"\n",
    "packages that generate train/test dataset\n",
    "\"\"\"\n",
    "from FormatData import generate_data, parser as data_parser\n",
    "\"\"\"\n",
    "packages that trains network\n",
    "\"\"\"\n",
    "from Training import train_network\n",
    "from utils.model import parser as model_parser\n",
    "\"\"\"\n",
    "packages that test network\n",
    "\"\"\"\n",
    "from Testing import test_model\n",
    "\"\"\"\n",
    "packages that produce the rejection shceme\n",
    "\"\"\"\n",
    "from filter_qa import parser as filter_parser, load_eddy\n",
    "\"\"\"\n",
    "package to store the intermediate result\n",
    "\"\"\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "packages that handle graphs\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from utils import calc_ssim\n",
    "%matplotlib inline\n",
    "def plot_loss(cmd):\n",
    "    \"\"\"\n",
    "    A function that used to plot the loss curve for the trained network.\n",
    "    Args:\n",
    "        cmd: String, the command line in the terminal\n",
    "    \"\"\"\n",
    "    args = model_parser().parse_args(cmd.split())\n",
    "    history = train_network(args)\n",
    "    # plt.plot(history.history['loss'])\n",
    "    # plt.plot(history.history['val_loss'])\n",
    "    # plt.title('model loss')\n",
    "    # plt.ylabel('loss')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.legend(['train', 'validation'], loc='upper left')\n",
    "    # plt.show()\n",
    "\n",
    "def show_slices(slices, grayscale=True):\n",
    "    \"\"\"\n",
    "    Function to display the slices\n",
    "\n",
    "    Args:\n",
    "        slices (list): a list of 2d ndarray that contains the data to be displayed\n",
    "        grayscale (bool, optional): True, if diplay grayscale img. Defaults to True.\n",
    "    \"\"\"    \n",
    "    fig, axes = plt.subplots(1, len(slices), figsize=(10,10))\n",
    "    cax = fig.add_axes([0, 0, .3, .3])\n",
    "    for i, slice in enumerate(slices):\n",
    "        # use grayscale for displaying ref and pred imgs:\n",
    "        if grayscale:\n",
    "            cmap = plt.get_cmap('gray')\n",
    "            cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "            cmap = LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            bounds = np.arange(0, 1.0, .01)\n",
    "            idx = np.searchsorted(bounds, 0)\n",
    "            bounds = np.insert(bounds, idx, 0)\n",
    "            norm = BoundaryNorm(bounds, cmap.N)\n",
    "            im = axes[i].imshow(slice.T, cmap=cmap, origin=\"lower\", interpolation='none', norm=norm)\n",
    "        else:\n",
    "            # define the colormap\n",
    "            cmap = plt.get_cmap('bwr')\n",
    "            # extract all colors from the .jet map\n",
    "            cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "            # create the new map\n",
    "            cmap = LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            # define the bins and normalize and forcing 0 to be part of the colorbar\n",
    "            # define the min and max to be -1 and +1 respectively\n",
    "            bounds = np.arange(-0.5, 0.5, .01)\n",
    "            idx = np.searchsorted(bounds, 0)\n",
    "            bounds = np.insert(bounds, idx, 0)\n",
    "            norm = BoundaryNorm(bounds, cmap.N)\n",
    "            im = axes[i].imshow(slice.T, cmap=cmap, origin=\"lower\", interpolation='none', norm=norm)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "def scale(img):\n",
    "    # for i in range(img.shape[0]):\n",
    "    #     for j in range(img.shape[1]):\n",
    "    #         img[i][j] = ((img[i][j]+1)/2)*255\n",
    "    return img\n",
    "\n",
    "def compare_simi(pred, ref):\n",
    "    return calc_ssim(pred, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The class for parsing command line arguments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    \"\"\"\n",
    "    a class generate parser for cmd line args\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "<h2>Random Rejection Scheme Setup</h2>\n",
    "Let N to be the tested number of retained volumes. For N, 100 subsampled schemes were drawn randomly from the full scheme (with the first b=0 volume and at least two different b values always included). Each subsampled schem was used to evaluate both techniques. For both 3D CNN and AMICO, N are 60, 40 and 30. We would give further undersampled scheme to 3D CNN, N are 20, 16 and 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bvals = '0 3000 1000 2000 2000 1000 2000 3000 1000 2000 3000 1000 2000 3000 1000 2000 3000 0 1000 3000 2000 1000 2000 3000 1000 2000 3000 1000 2000 3000 1000 2000 3000 0 1000 3000 2000 1000 1000 2000 3000 2000 3000 1000 2000 3000 1000 2000 3000 0 1000 2000 3000 1000 2000 3000 1000 2000 3000 1000 2000 3000 2000 1000 3000 0 1000 3000 2000 3000 1000 2000 1000 2000 3000 3000 1000 2000 1000 2000 3000 0 1000 3000 2000 1000 2000 2000 1000 3000 3000 1000 2000 3000 3000 1000'\n",
    "# bvals = np.asarray(bvals.split())\n",
    "# b1000 = np.where(bvals=='1000')\n",
    "# b2000 = np.where(bvals=='2000')\n",
    "# b3000 = np.where(bvals=='3000')\n",
    "# b0 = np.where(bvals=='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list contains the N for each tested retained volume\n",
    "# N = [60, 40, 30, 20, 16, 12]\n",
    "# random_dict = {60:[], 40:[], 30:[], 20:[], 16:[], 12:[]}\n",
    "# random_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated the random scheme for each tested volume\n",
    "# for n in N:\n",
    "#     # for each retained volume, we create 100 subsamples\n",
    "#     for i in range(100):\n",
    "#         ones = np.ones(n-1) # 1 = retained\n",
    "#         zeros = np.zeros(96-n)\n",
    "\n",
    "#         scheme = np.random.choice(np.concatenate([ones,zeros]), 95, replace=False)\n",
    "#         # always select the first b=0 volume\n",
    "#         scheme = np.insert(scheme, 0, 1.0)\n",
    "#         select_index = np.where(scheme==1)\n",
    "#         # check if 1000 is included\n",
    "#         has1000 = np.intersect1d(select_index, b1000)\n",
    "#         has2000 = np.intersect1d(select_index, b2000)\n",
    "#         has3000 = np.intersect1d(select_index, b3000)\n",
    "#         if len(has1000) == 0:\n",
    "#             scheme[b1000[0]] = 1\n",
    "#             if len(has2000) > 1:\n",
    "#                 # remove a 2000 b val volume\n",
    "#                 scheme[has2000[0]] = 0\n",
    "#             else:\n",
    "#                 scheme[has3000[0]] = 0\n",
    "#         if len(has2000) == 0:\n",
    "#             scheme[b2000[0]] = 1\n",
    "#             if len(has1000) > 1:\n",
    "#                 # remove a 2000 b val volume\n",
    "#                 scheme[has1000[0]] = 0\n",
    "#             else:\n",
    "#                 scheme[has3000[0]] = 0\n",
    "#         if len(has3000) == 0:\n",
    "#             scheme[b3000[0]] = 1\n",
    "#             if len(has1000) > 1:\n",
    "#                 # remove a 2000 b val volume\n",
    "#                 scheme[has1000[0]] = 0\n",
    "#             else:\n",
    "#                 scheme[has2000[0]] = 0\n",
    "#         scheme = ' '.join(map(str, scheme))\n",
    "#         random_dict[n].append(scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the rejection scheme\n",
    "# random_60_file = 'random_60.pickle'\n",
    "# random_40_file = 'random_40.pickle'\n",
    "# random_30_file = 'random_30.pickle'\n",
    "# random_20_file = 'random_20.pickle'\n",
    "# random_16_file = 'random_16.pickle'\n",
    "# random_12_file = 'random_12.pickle'\n",
    "\n",
    "# with open(random_60_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[60], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(random_40_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[40], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(random_30_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[30], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(random_20_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[20], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(random_16_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[16], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(random_12_file, 'wb') as handle:\n",
    "#     pickle.dump(random_dict[12], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the random files have already been created, just read those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dict = {60:[], 40:[], 30:[], 20:[], 16:[], 12:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fp = '/home/vw/Data/IndividualProject/MedICSS2021_/Net/new_exp2_results'\n",
    "\n",
    "with open(os.path.join(random_fp, 'random_60.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[60].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "with open(os.path.join(random_fp, 'random_40.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[40].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "with open(os.path.join(random_fp, 'random_30.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[30].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "with open(os.path.join(random_fp, 'random_20.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[20].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "with open(os.path.join(random_fp, 'random_16.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[16].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "with open(os.path.join(random_fp, 'random_12.pickle'), 'rb') as file:\n",
    "    while True:\n",
    "        try:\n",
    "            random_dict[12].append(pickle.load(file))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "random_dict[60] = np.array(random_dict[60][0])\n",
    "random_dict[40] = np.array(random_dict[40][0])\n",
    "random_dict[30] = np.array(random_dict[30][0])\n",
    "random_dict[20] = np.array(random_dict[20][0])\n",
    "random_dict[16] = np.array(random_dict[16][0])\n",
    "random_dict[12] = np.array(random_dict[12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rmse_dict = {60:[], 40:[], 30:[], 20:[], 16:[], 12:[]}\n",
    "cnn_ssim_dict = {60:[], 40:[], 30:[], 20:[], 16:[], 12:[]}\n",
    "cnn_percent_dict = {60:[], 40:[], 30:[], 20:[], 16:[], 12:[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "<h2>Training</h2>\n",
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the networks\n",
    "layers = 4 # the number of hidden layers; this is the optimal number of layer\n",
    "lr = 0.0001 # the learning rate\n",
    "patch_size = 3 # the size of patches for 2D and 3D CNN\n",
    "batch = 256 # the batch size\n",
    "epoch = 100 # the number of epoches for training\n",
    "model = 'conv3d'\n",
    "ltype = 'A'\n",
    "\n",
    "# we firstly study subject s02, use s01 as a seperate training dataset\n",
    "study_subject = 's03_still_reg' # the subject we want to study\n",
    "sep_train_subject = 's01_still' # this is separate training candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 60</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[60])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[60][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[60].append(RMSE)\n",
    "    cnn_ssim_dict[60].append(SSIM)\n",
    "    cnn_percent_dict[60].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_60_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_60_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_60_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_60_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_60_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_60_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_60_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_60_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_60_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[60][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[60])//2, len(random_dict[60])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[60][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[60].append(RMSE)\n",
    "    cnn_ssim_dict[60].append(SSIM)\n",
    "    cnn_percent_dict[60].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_60_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_60_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_60_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_60_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_60_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_60_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_60_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_60_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_60_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[60][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 40</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[40])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[40][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[40].append(RMSE)\n",
    "    cnn_ssim_dict[40].append(SSIM)\n",
    "    cnn_percent_dict[40].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_40_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_40_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_40_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_40_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_40_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_40_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_40_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_40_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_40_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[40][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[40])//2, len(random_dict[40])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[40][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[40].append(RMSE)\n",
    "    cnn_ssim_dict[40].append(SSIM)\n",
    "    cnn_percent_dict[40].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_40_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_40_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_40_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_40_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_40_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_40_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_40_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_40_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_40_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[40][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 30</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[30])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[30][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[30].append(RMSE)\n",
    "    cnn_ssim_dict[30].append(SSIM)\n",
    "    cnn_percent_dict[30].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_30_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_30_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_30_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_30_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_30_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_30_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_30_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_30_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_30_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[30][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[30])//2, len(random_dict[30])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[30][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[30].append(RMSE)\n",
    "    cnn_ssim_dict[30].append(SSIM)\n",
    "    cnn_percent_dict[30].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_30_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_30_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_30_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_30_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_30_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_30_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_30_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_30_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_30_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[30][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 20</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[20])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[20][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[20].append(RMSE)\n",
    "    cnn_ssim_dict[20].append(SSIM)\n",
    "    cnn_percent_dict[20].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_20_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_20_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_20_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_20_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_20_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_20_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_20_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_20_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_20_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[20][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[20])//2, len(random_dict[20])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[20][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[20].append(RMSE)\n",
    "    cnn_ssim_dict[20].append(SSIM)\n",
    "    cnn_percent_dict[20].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_20_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_20_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_20_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_20_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_20_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_20_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_20_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_20_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_20_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[20][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 16</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[16])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[16][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[16].append(RMSE)\n",
    "    cnn_ssim_dict[16].append(SSIM)\n",
    "    cnn_percent_dict[16].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_16_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_16_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_16_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_16_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_16_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_16_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_16_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_16_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_16_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[16][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(random_dict[16])//2, len(random_dict[16])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    movefile = random_dict[16][i]\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[16].append(RMSE)\n",
    "    cnn_ssim_dict[16].append(SSIM)\n",
    "    cnn_percent_dict[16].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_16_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_16_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_16_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_16_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_16_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_16_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_16_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_16_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_16_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[16][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N = 12</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(random_dict[12])//2):\n",
    "for i in range(len(random_dict[12])//2):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    # movefile = random_dict[12][i]\n",
    "    movefile = random_dict[12][i]\n",
    "    # movefile = movefile.split()\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[12].append(RMSE)\n",
    "    cnn_ssim_dict[12].append(SSIM)\n",
    "    cnn_percent_dict[12].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_12_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_12_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_12_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_12_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_12_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_12_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_12_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_12_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_12_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[12][:50]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(random_dict[12])//2, len(random_dict[12])):\n",
    "for i in range(len(random_dict[12])//2, len(random_dict[12])):\n",
    "    # we have 100 movefiles for each retained tested volume\n",
    "    # Training\n",
    "    # movefile = random_dict[12][i]\n",
    "    movefile = random_dict[12][i]\n",
    "    # movefile = movefile.split()\n",
    "    cnntrain_cmd = '--train_subjects ' + sep_train_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --lr ' + str(lr) + ' --batch ' + str(batch) + ' --patch_size ' + str(patch_size)\\\n",
    "               + ' --epoch ' + str(epoch) + ' --movefile ' + movefile + ' --train'\n",
    "    plot_loss(cnntrain_cmd)\n",
    "    # Testing\n",
    "    cnntest_cmd = '--test_subjects ' + study_subject + ' --model ' + model + ' --layer ' + str(layers) + ' --label_type ' + ltype + ' --movefile ' + movefile\n",
    "    cnntest_args = model_parser().parse_args(cnntest_cmd.split())\n",
    "    RMSE, SSIM, Percent = test_model(cnntest_args)\n",
    "    # Add results to the cnn dict\n",
    "    cnn_rmse_dict[12].append(RMSE)\n",
    "    cnn_ssim_dict[12].append(SSIM)\n",
    "    cnn_percent_dict[12].append(Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_rmse_12_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_rmse_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_12_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_rmse_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_rmse_12_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_rmse_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_ssim_12_ndi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_ssim_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_12_odi_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_ssim_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_ssim_12_fwf_'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_ssim_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cnn_percent_12_ndi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[0] for item in cnn_percent_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_12_odi'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[1] for item in cnn_percent_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('cnn_percent_12_fwf'+str(i)+'.pickle', 'wb') as file:\n",
    "    pickle.dump([item[2] for item in cnn_percent_dict[12][50:]], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d00557ecee9d041f78bfa618225def395bc332f64d1935e44218a847c69687c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
